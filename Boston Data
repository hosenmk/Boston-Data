import pandas as pd
df = pd.read_csv('Boston.csv')

#adding new rows
Newrows=[[0.069,10,2.3,0,0.53,6.5,65.2,4.01,1,290,15,395,4.9,24],[1.69,11,2.31,0,0.51,6.51,65.21,4.11,1,291,16,396,4.91,25.3],
[1.68,12,2.41,0,0.61,6.61,65.11,4.01,1,292,14,391,4.21,25.2],[1.67,13,2.51,0,0.41,6.51,65.31,4.21,1,293,15,393,4.31,25.1],
[1.66,14,2.41,0,0.71,6.51,65.41,4.11,1,294,17,392,4.41,25.2]]

df1=pd.DataFrame(Newrows,columns = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',
'ptratio', 'b', 'lstat', 'medv'])

Newdf=pd.concat([df, df1],ignore_index=True)
Newdf.to_csv('ModifiedBoston.csv', index=False)

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import seaborn as sns

df = pd.read_csv("ModifiedBoston.csv)
df.describe()

plt.figure(figsize=(10, 6))
import matplotlib.pyplot as plt
plt.bar(df.crim,df.medv,color='blue')
plt.xlabel('Crime Rate')
plt.ylabel('Price of the House')
plt.title('Crime rate vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df.indus,df.medv,color='indigo')
plt.xlabel('Non-Retail business proportion')
plt.ylabel('Price of the House')
plt.title('Non-Retail business proportion vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df.nox,df.medv,color='violet')
plt.xlabel('Nitric Oxide concentration')
plt.ylabel('Price of the House')
plt.title('Nitric Oxide concentration vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df.rm,df.medv,color='green')
plt.xlabel('Avg. No. of Rooms')
plt.ylabel('Price of the House')
plt.title('Avg. No. of Rooms vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df.ptration,df.medv,color='grey')
plt.xlabel('Pupil-Teacher ratio')
plt.ylabel('Price of the House')
plt.title('Pupil Teacher ratio vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df.lstat,df.medv,color='orange')
plt.xlabel('% lower population')
plt.ylabel('Price of the House')
plt.title('% lower population vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(df.dis,df.medv)
plt.xlabel('Distance')
plt.ylabel('Price of the House')
plt.title('Distance vs Price of house')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['rm'], bins=30, kde=True)
plt.title('Distribution of Rooms (RM) in Boston Housing Data')
plt.xlabel('Number of Rooms (RM)')
plt.ylabel('Frequency')

correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Boston Housing Data')
plt.show()

df = pd.read_csv('Boston.csv')
def category_label(medv):
  if medv > 20:
    return 1
  else:
    return 0
df['category'] = df['medv'].map(category_label)

df['category'] = df['medv'].map(category_lable)
df.drop(columns=["medv"], inplace=True)
df['category']=df['category'].map({0:'Low', 1:'High'})
sns.pairplot(df, hue='category', plot_kws={'alpha': 0.5}, palette={'HIgh':'blue', 'Low': 'red'})
plt.subtitle('Pair Plots (Boston Data Variables)', y=1.02)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlob.mlab as mlab
import seaborn as sns
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('ModifiedBoston.csv')
#Modified with five new rows as directed

#Mapping Function
def category_label(medv):
  if medv > 20:
    return 1
  else:
    return 0
df['category'] = df['medv'].map(category_label)
df.drop(columns=['medv'], inplace=True)

cols = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']
x=sm.add_constant(df[cols])
y = df['category']
logitmodel=sm.Logit(y,x)
result=logitmodel.fit()
print(result.summary2())

cols_significant = ['indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']
x=sm.add_constant(df[cols_significant])
logitmeodel=sm.Logit(y,x)
result=logitmodel.fit()
print(result.summary2())

#Now al the re3maining 11 variables along with intercept are found sgnificant considering a<0.05
print(result.params)

#Estimate of Probability of HIgh Price:
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score
from sklearn.preprocessing import StandardScaler

x = df[cols_significant]
y =df['category']

# Split the dataset into training (70%) and test (30%) sets with seed 101
X_train, X_test, y_train, y_test, = train_test_split(x, y, test_size=0.3, random_state=101)

scaler = StandardScaler()
X_train = scaler.fit_transform(x_train)
X_test = scaler.transform(X_test)

# Logistic Regeression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg_predictions = log_reg.predict(X_test)

accuracy_log_reg = accuracy_score(y_test, log_reg_predictions)
conf_matrix_log_reg = confusion_kmatrix(y_test, log_reg_predictions)
sensitiviry_log_reg = conf_matrix_log_reg[1, 1] / (conf_matrix_log_reg[1, 1] + conf_matrix_log_reg[1, 0])
Specificity_log_reg = conf_matrix_log_reg[0, 0] / (conf_matrix_log_reg[0, 0] + conf_matrix_log_reg[0, 1])
log_reg_probs = log_reg.predict_proba(X_test)[:. 1]
fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, log_reg_probs)
auc_log_reg = roc_curve_score(y_test, log_reg_probs)

Print("Logistics Regression:")
Print(c"Confusion Matrix:\n{conf_matrix_log_reg}\n")
print(f"Accuracy: {accuracy_log_reg:.2f}")
print(f"Sensitivity: {sensitivity_log_reg:.2f}")
print(f"Specificity: {specificity_log_reg:.2f}")
print(f"AUC: {auc_log_reg:.2f}\n")

# Decision Trees
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)
dt_predictions = decision_tree.predict(X_test)

accuracy_dt = accuracy_score(y_test, dt_predictions)
conf_matrix_dt = confusion_matrix(y_test, dt_predictions)

sensitivity_dt = conf_matrix_dt[1, 1] / (conf_matrix_dt[1, 1] + conf_matrix_dt[1, 0])
specificity_dt = conf_matrix_dt[0, 0] / (conf_matrix_dt[0, 0] + conf_matrix_dt[0, 1])

dt_probs = decision_tree.predict_proba(X_test)[:, 1]
fpr_dt, tpr_dt, _ = roc_curve(y_test, dt_probs)
auc_dt = roc_auc_score(y_test, dt_probs)

print("Decision Tree:")
print(f"Confusion Matrix:\n{conf_matrix_dt}\n")
print(f"Accuracy: {accuracy_dt:.2f}")
print(f"Sensitivity: {sensitivity_dt:.2f}")
print(f"Specificity: {specificity_dt:.2f}")
print(f"AUC: {auc_dt:.2f}\n")

#Random Forest
random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)
rf_prediction = random_forest.predict(X_test)
accuracy_rf = accuracy_score(y_test, rf_predictions)
conf_matrix_rf = confusion_matrix(y_test, rf_predictions)

sensitivity_rf = conf_matrix_rf[1, 1] / (conf_matrix_rf[1, 1] + conf_matrix_rf[1, 0])
specificity_rf = conf_matrix_rf[0, 0] / (conf_matrix_rf[0, 0] + conf_matrix_rf[0, 1])
rf_probs = random_forest.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
auc_rf = roc_auc_score(y_test, rf_probs)

print("Random Forest:")
print(f"Confusion Matrix:\n{conf_matrix_svm}\n")
print(f"Accuracy: {accuracy_svm:.2f}")
print(f"Sensitivity: {sensitivity_svm:.2f}")
print(f"Specificity: {specificity_svm:.2f}")
print(f"AUC: {auc_svm:.2f}\n")

# Plot ROC curves for each model
plt.figure(figsize=(8, 6))
plt.plot(fpr_log_reg, tpr_log_reg, label=f'Logistic Regression (AUC = {auc_log_reg:.2f})')
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.2f})')
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rg:.2f})')
plt.plot(fpr_svm tpr_svm, label=f'SVM (AUC = {auc_svm:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Lists Containing evaluation metrics for each model
accuracy_scores = [accuracy_log_reg, accuracy_dt, accuracy_rf, accuracy_svm]
sensitivity_scores = [sensitivity_log_reg, sensitivity_dt, sensitivity_rf, sensitivity_svm]
specificity_scores = [specificity_log_reg, specificity_dt, specificity_rf, specificity_svm]
auc_scores = [auc_log_reg, auc_dt, auc_rf, auc_svm]

models = ['Logistics Regression', 'Decision Tree', 'Random forest', 'SVM']

print("Comparative Evaluation of Models")
print(f"{'Model':<20} {'Accuracy':<15} {'Sensitivity':<15} {'Specificity':<15} {'AUC':<15}")
for i, model in enumerate(models):
  print(f"{model:<20} {accuracy_scores[i]:<15.2f} {sensitivity_socres[i]:<15.2f} {specificity_score[i]:<15.2f} {auc_scores[i]:<15.2f}")



